На самом базовом уровне компьютер работает только с нулями и единицами — это так называемый двоичный код. Каждую единицу или ноль называют битом (от binary digit — «двоичная цифра»).

Любые данные — изображения, музыка, текст — в компьютере представлены просто как последовательность битов.

Например, привычные нам числа из десятичной системы можно представить в двоичной форме:

- 0 → `0`
- 1 → `1`
- 2 → `10`
- 3 → `11`
- 4 → `100`
- 5 → `101`

## Как закодировать текст?

Компьютер не «понимает» текст. Чтобы работать с буквами и другими символами, их тоже нужно превратить в числа. Это делают с помощью кодировок — таблиц, в которых каждому символу соответствует определённое число.

Самый простой способ — пронумеровать буквы, начиная с 1:

- `a` → `1`
- `b` → `2`
- `c` → `3`
- ...
- `z` → `26`

Теперь мы можем представить слово hello как набор чисел:

```
h e l l o
↓ ↓ ↓ ↓ ↓
8 5 12 12 15
```


А *good* превращается в:

```
g o o d
↓ ↓ ↓ ↓
7 15 15 4
```

Программа не знает, что это слово. Она просто видит: «нужно отобразить символ с кодом 8, потом с кодом 5 и т.д.»

## ASCII: первая массовая кодировка

Первые компьютеры работали в основном с английским языком. Для него в 1960-х годах придумали таблицу ASCII — она включала 128 символов: латинский алфавит, цифры, знаки препинания, спецсимволы (@, #, !, \n) и управляющие коды.

Этого хватало для первых программ, но не для всего мира.

Когда компьютеры начали использовать в других странах, возникла проблема: в ASCII нет кириллицы, иероглифов, арабского письма, ударений, валютных символов и т. д.

Каждая страна или компания начала делать свою кодировку на основе ASCII:

- Windows придумала Windows-1251 для русского
- Apple — Mac Roman
- IBM — свои версии
- Страны Восточной Европы, Азии и Ближнего Востока — свои

Все эти кодировки были несовместимы между собой. Код 226 в одной мог быть буквой é, в другой — и, в третьей — вообще техническим символом. Это приводило к настоящему хаосу.

## Как выглядели проблемы с кодировками

Если вы видите в тексте вот это:

```
ÐÑÐ¸Ð²ÐµÑ!
```

или

```
���� �������� ������
```

— значит, программа неправильно определила кодировку текста. Она получила последовательность байтов, но прочитала их не той таблицей.

Это было нормой в 1990–2000-х. Одна программа писала текст в Windows-1251, другая читала его как ISO-8859-1, и в результате получался мусор.

## Unicode и UTF-8: конец бардака

Чтобы всё починить, в 1990-х начали создавать универсальную таблицу Unicode, которая содержит символы всех письменных систем мира — от латинского и кириллицы до китайского, арабского, математических знаков, древнеегипетских и даже эмодзи.

Внутри Unicode есть несколько форматов хранения. Самый распространённый — UTF-8. Он компактно кодирует английские символы, но может расширяться под любые другие.

Сегодня UTF-8 — это стандарт по умолчанию в интернете, Python, Linux, базах данных и редакторах кода.

## Зачем это знать программисту?

- Вы будете работать с текстом: ошибки кодировки по-прежнему случаются, особенно при чтении файлов, обработке данных, взаимодействии с API и базами данных.
- Python по умолчанию использует UTF-8, но иногда приходится явно указывать кодировку при чтении файлов:
- Нужно уметь диагностировать проблемы. Например, если видите «кракозябры», это почти наверняка ошибка кодировки.
